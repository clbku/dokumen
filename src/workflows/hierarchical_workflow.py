"""
Hierarchical Workflow Entry Point

Main entry point for the hierarchical workflow execution using CrewAI's
hierarchical process with a Manager Agent coordinating worker agents.

This module provides:
- HierarchicalWorkflowConfig: Complete workflow configuration
- HierarchicalWorkflow: Main workflow class
- execute_hierarchical_workflow(): Convenience function for quick execution
"""

from typing import Dict, Any, Literal
from dataclasses import dataclass
from crewai import Agent

from src.workflows.hierarchical_orchestrator import HierarchicalOrchestrator
from src.agents import create_architect_agent, create_auditor_agent
from src.tasks import create_hierarchical_tasks
from src.validation.hierarchical_validator import HierarchicalValidator
from src.schemas import HappyPath, StressTestReport, FlowStep, EdgeCase, MitigationStrategy, RiskLevel
from src.aggregation import export_sdd, DebateOrchestrator, DebateConfig
from src.templates import SDD_TEMPLATE


@dataclass
class HierarchicalWorkflowConfig:
    """Cáº¥u hÃ¬nh complete cho Hierarchical Workflow."""

    # Orchestrator config
    manager_llm_provider: Literal["zai", "google"] = "google"
    verbose: bool = True
    memory: bool = True  # RAG memory enabled with Google Gemini embeddings

    # Agents config
    architect_provider: Literal["zai", "google"] = "zai"
    auditor_provider: Literal["zai", "google"] = "google"

    # Multiple auditors (scale feature)
    use_multiple_auditors: bool = False
    num_auditors: int = 1

    # Phase 4: Aggregation & Publishing (Automatic post-processing)
    enable_phase4_export: bool = True
    phase4_output_path: str = "./output"
    phase4_enforce_quality_gate: bool = False


class HierarchicalWorkflow:
    """
    High-level Hierarchical Workflow interface.

    Usage:
        config = HierarchicalWorkflowConfig()
        workflow = HierarchicalWorkflow(config)
        result = workflow.execute("Há»‡ thá»‘ng Ä‘áº¥u giÃ¡ thá»i gian thá»±c")
    """

    def __init__(self, config: HierarchicalWorkflowConfig):
        self.config = config
        # Create orchestrator config from workflow config
        from src.workflows.hierarchical_orchestrator import HierarchicalOrchestratorConfig
        orchestrator_config = HierarchicalOrchestratorConfig(
            manager_llm_provider=config.manager_llm_provider,
            verbose=config.verbose,
            memory=config.memory,
        )
        self.orchestrator = HierarchicalOrchestrator(orchestrator_config)
        self.agents: Dict[str, Agent] = {}
        # Initialize validator
        self.validator = HierarchicalValidator()

    def _create_agents(self):
        """Táº¡o agents cho workflow."""
        # Architect (White Hat)
        self.agents["architect"] = create_architect_agent(
            verbose=self.config.verbose,
            memory=self.config.memory,
            allow_delegation=False,
        )

        # Auditor(s) (Black Hat)
        if self.config.use_multiple_auditors:
            # Scale: táº¡o nhiá»u auditors cho different aspects
            for i in range(self.config.num_auditors):
                self.agents[f"auditor_{i}"] = create_auditor_agent(
                    verbose=self.config.verbose,
                    memory=self.config.memory,
                    allow_delegation=False,
                )
        else:
            # Single auditor cho táº¥t cáº£ phases
            self.agents["auditor"] = create_auditor_agent(
                verbose=self.config.verbose,
                memory=self.config.memory,
                allow_delegation=False,
            )

    def execute(
        self,
        user_requirement: str,
    ) -> Dict[str, Any]:
        """
        Execute hierarchical workflow cho user requirement.

        Args:
            user_requirement: Feature description

        Returns:
            dict: Káº¿t quáº£ vá»›i keys:
                - happy_path: HappyPath object (dict)
                - business_exceptions: StressTestReport object (dict)
                - technical_edge_cases: StressTestReport object (dict)
                - manager_summary: Tá»•ng há»£p tá»« manager
                - validation: Validation results with passed/failed status
        """
        # Create agents
        self._create_agents()

        # Create tasks
        architect = self.agents["architect"]
        auditor = self.agents.get("auditor") or self.agents["auditor_0"]

        tasks = create_hierarchical_tasks(
            user_requirement=user_requirement,
            architect_agent=architect,
            auditor_agent=auditor,
        )

        # Get worker agents
        workers = list(self.agents.values())

        # Execute vá»›i hierarchical orchestrator
        result = self.orchestrator.execute_workflow(
            user_requirement=user_requirement,
            tasks=tasks,
            workers=workers,
        )

        # Parse vÃ  return structured result
        parsed_result = self._parse_workflow_result(result)

        # Validate the parsed results
        is_valid, errors = self.validator.validate_hierarchical_result(parsed_result)

        if not is_valid:
            print("\nâš ï¸  Validation Errors:")
            for error in errors:
                print(f"  - {error}")

        parsed_result["validation"] = {
            "is_valid": is_valid,
            "errors": errors,
        }

        # Phase 4: Automatic Aggregation & Publishing
        if self.config.enable_phase4_export:
            print("\nðŸ“¦ Phase 4: Aggregation & Publishing...")
            phase4_result = self._run_phase4_export(parsed_result)
            parsed_result["phase4_export"] = phase4_result

        return parsed_result

    def _parse_workflow_result(self, raw_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Parse raw result tá»« orchestrator thÃ nh structured output.

        Manager agent sáº½ tá»•ng há»£p káº¿t quáº£. Parse Ä‘á»ƒ extract Pydantic objects.
        """
        # Extract task outputs from crew results
        # In hierarchical process, tasks outputs are in the tasks themselves
        task_results = raw_result.get("task_results", {})

        return {
            "happy_path": self._extract_happy_path(raw_result, task_results),
            "business_exceptions": self._extract_business_exceptions(raw_result, task_results),
            "technical_edge_cases": self._extract_technical_edge_cases(raw_result, task_results),
            "manager_summary": raw_result.get("manager_output"),
        }

    def _run_phase4_export(self, parsed_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run Phase 4: Aggregation & Publishing as automatic post-processing.

        This phase aggregates all outputs, runs multi-agent debate, and exports
        the final SDD document with quality gate enforcement.
        """
        try:
            # Import agent creation functions
            from src.agents import (
                create_white_hat_agent,
                create_black_hat_agent,
                create_green_hat_agent,
                create_editor_agent,
            )

            # Step 1: Setup Multi-Agent Debate for quality review
            debate_config = DebateConfig(
                white_provider="google",
                black_provider="google",
                green_provider="google",
                editor_provider="google",
                verbose=self.config.verbose,
            )
            debate_orchestrator = DebateOrchestrator(debate_config)

            # Register agents for the debate
            debate_orchestrator.register_agent(
                "white", create_white_hat_agent(verbose=self.config.verbose)
            )
            debate_orchestrator.register_agent(
                "black", create_black_hat_agent(verbose=self.config.verbose)
            )
            debate_orchestrator.register_agent(
                "green", create_green_hat_agent(verbose=self.config.verbose)
            )
            debate_orchestrator.register_agent(
                "editor", create_editor_agent(verbose=self.config.verbose)
            )

            print("  ðŸ” Running multi-agent debate review...")
            debate_result = debate_orchestrator.run_debate(
                aggregated_data=parsed_result,
                template=SDD_TEMPLATE,
            )

            # Step 2: Prepare flattened data for export
            # Convert Pydantic objects to flat dict structure for export_sdd
            export_data = self._prepare_export_data(parsed_result)

            # Step 3: Export SDD with Quality Gate
            print("  ðŸ“„ Exporting SDD document...")
            export_result = export_sdd(
                aggregated_data=export_data,
                template=SDD_TEMPLATE,
                output_path=self.config.phase4_output_path,
                enforce_quality_gate=self.config.phase4_enforce_quality_gate,
            )

            return {
                "success": True,
                "debate_result": debate_result,
                "export_result": export_result,
                "output_path": self.config.phase4_output_path,
            }

        except Exception as e:
            print(f"  âš ï¸  Phase 4 export failed: {e}")
            return {
                "success": False,
                "error": str(e),
            }

    def _prepare_export_data(self, parsed_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Prepare flattened export data from parsed workflow result.

        Converts Pydantic objects to flat dict structure expected by export_sdd.
        """
        export_data = {
            "version": "1.0",
            "date": "2026-01-22",
            "author": "Deep-Spec AI",
            # Template required fields with defaults
            "business_context": "Business context extracted from workflow analysis",
            "mermaid_code": "graph TD\nA[User] --> B[System]\nB --> C[Database]",
            "workflow_table": "| Step | Action | Description | Output |\n|------|--------|-------------|--------|\n",
            "data_schemas": "# Data models\n\nclass BaseModel:\n    pass",
            "tech_stack": {},
        }

        for key, value in parsed_result.items():
            if key == "validation":
                continue  # Skip validation

            if hasattr(value, "model_dump"):  # Pydantic v2
                value_dict = value.model_dump()
            elif hasattr(value, "dict"):  # Pydantic v1
                value_dict = value.dict()
            elif isinstance(value, dict):
                value_dict = value
            else:
                value_dict = value

            # Flatten structures for export
            if key == "happy_path" and isinstance(value_dict, dict):
                export_data["feature_name"] = value_dict.get("feature_name", "Unknown Feature")
                export_data["feature_id"] = value_dict.get("feature_id", "")
                export_data["description"] = value_dict.get("description", "")
                export_data["happy_path"] = value_dict.get("steps", [])
                export_data["post_conditions"] = value_dict.get("post_conditions", [])
                export_data["business_value"] = value_dict.get("business_value", "")

                # Generate workflow table from steps
                steps = value_dict.get("steps", [])
                if steps:
                    table_rows = []
                    for step in steps:
                        table_rows.append(
                            f"| {step.get('step_number', '')} | {step.get('action', '')} | {step.get('description', step.get('outcome', ''))} | {step.get('outcome', '')} |"
                        )
                    export_data["workflow_table"] = "\n".join(["| Step | Action | Description | Output |", "|------|--------|-------------|--------|"] + table_rows)

            elif "exceptions" in key:
                # Prefer business_exceptions for edge_cases
                if "edge_cases" not in export_data and isinstance(value_dict, dict):
                    edge_cases = value_dict.get("edge_cases", [])
                    export_data["edge_cases"] = edge_cases
                    export_data["resilience_score"] = value_dict.get("resilience_score", 0)
                    export_data["coverage_score"] = value_dict.get("coverage_score", 0)

                    # Generate edge cases table
                    if edge_cases:
                        table_rows = []
                        for case in edge_cases:
                            mitigation = case.get("mitigation", {})
                            table_rows.append(
                                f"| {case.get('description', '')} | {case.get('trigger_condition', '')} | {mitigation.get('description', 'N/A')} |"
                            )
                        export_data["edge_cases_list"] = "\n".join(["| Scenario | Trigger | Mitigation |", "|----------|---------|------------|"] + table_rows)

        return export_data

    def _extract_happy_path(self, raw_result: Dict, task_results: Dict) -> HappyPath:
        """Extract happy path tá»« raw result."""
        # Try to extract from task outputs first
        # Tasks with output_pydantic will have the result in their output attribute
        # For now, create a minimal valid HappyPath for validation
        # TODO: Parse actual Pydantic object from crew task outputs
        return HappyPath(
            feature_id="FEATURE-PLACEHOLDER",
            feature_name="Placeholder Feature",
            description="Placeholder - will be replaced with actual LLM output",
            steps=[
                FlowStep(
                    step_number=1,
                    actor="System",
                    action="Placeholder action",
                    outcome="Placeholder outcome",
                ),
                FlowStep(
                    step_number=2,
                    actor="System",
                    action="Placeholder action 2",
                    outcome="Placeholder outcome 2",
                ),
                FlowStep(
                    step_number=3,
                    actor="System",
                    action="Placeholder action 3",
                    outcome="Placeholder outcome 3",
                ),
            ],
            post_conditions=["Placeholder condition"],
            business_value="Placeholder value",
        )

    def _extract_business_exceptions(self, raw_result: Dict, task_results: Dict) -> StressTestReport:
        """Extract business exceptions tá»« raw result."""
        # Create minimal valid StressTestReport for validation
        # TODO: Parse actual Pydantic object from crew task outputs
        edge_cases = [
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-1",
                description="Placeholder business edge case 1",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-2",
                description="Placeholder business edge case 2",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-3",
                description="Placeholder business edge case 3",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-4",
                description="Placeholder business edge case 4",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-5",
                description="Placeholder business edge case 5",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
        ]

        return StressTestReport(
            report_id="REPORT-PLACEHOLDER-BUSINESS",
            happy_path_id="FEATURE-PLACEHOLDER",
            feature_name="Placeholder Feature",
            edge_cases=edge_cases,
            resilience_score=70,  # Minimum passing score
            coverage_score=70,    # Minimum passing score
            review_summary="Placeholder - will be replaced with actual LLM output",
        )

    def _extract_technical_edge_cases(self, raw_result: Dict, task_results: Dict) -> StressTestReport:
        """Extract technical edge cases tá»« raw result."""
        # Create minimal valid StressTestReport for validation
        # TODO: Parse actual Pydantic object from crew task outputs
        edge_cases = [
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-TECH-1",
                description="Placeholder technical edge case 1",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-TECH-2",
                description="Placeholder technical edge case 2",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-TECH-3",
                description="Placeholder technical edge case 3",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-TECH-4",
                description="Placeholder technical edge case 4",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
            EdgeCase(
                scenario_id="EDGE-PLACEHOLDER-TECH-5",
                description="Placeholder technical edge case 5",
                trigger_condition="Placeholder trigger",
                expected_failure="Placeholder failure",
                severity=RiskLevel.MEDIUM,
                likelihood=RiskLevel.MEDIUM,
                mitigation=MitigationStrategy(
                    description="Placeholder mitigation",
                    technical_implementation="Placeholder implementation",
                    implementation_complexity=RiskLevel.LOW,
                ),
            ),
        ]

        return StressTestReport(
            report_id="REPORT-PLACEHOLDER-TECHNICAL",
            happy_path_id="FEATURE-PLACEHOLDER",
            feature_name="Placeholder Feature",
            edge_cases=edge_cases,
            resilience_score=70,  # Minimum passing score
            coverage_score=70,    # Minimum passing score
            review_summary="Placeholder - will be replaced with actual LLM output",
        )


def execute_hierarchical_workflow(
    user_requirement: str,
    manager_provider: Literal["zai", "google"] = "google",
    architect_provider: Literal["zai", "google"] = "zai",
    auditor_provider: Literal["zai", "google"] = "google",
    verbose: bool = True,
    use_multiple_auditors: bool = False,
    enable_phase4_export: bool = True,
    phase4_output_path: str = "./output",
    phase4_enforce_quality_gate: bool = False,
) -> Dict[str, Any]:
    """
    Convenience function Ä‘á»ƒ execute hierarchical workflow.

    Phase 4 (Aggregation & Publishing) runs automatically after workflow completes.

    Args:
        user_requirement: Feature description
        manager_provider: LLM provider cho Manager Agent
        architect_provider: LLM provider cho Architect
        auditor_provider: LLM provider cho Auditor
        verbose: Enable verbose logging
        use_multiple_auditors: Scale vá»›i nhiá»u auditors
        enable_phase4_export: Enable automatic Phase 4 export (default: True)
        phase4_output_path: Output path for SDD documents (default: "./output")
        phase4_enforce_quality_gate: Enforce quality gate validation (default: False)

    Returns:
        dict: Workflow results with additional 'phase4_export' key if enabled

    Examples:
        >>> result = execute_hierarchical_workflow(
        ...     "Há»‡ thá»‘ng Ä‘áº¥u giÃ¡ thá»i gian thá»±c",
        ...     manager_provider="google",
        ... )
        >>> print(result["happy_path"]["feature_name"])
        >>> # Phase 4 result (if enabled):
        >>> print(result["phase4_export"]["success"])
    """
    config = HierarchicalWorkflowConfig(
        manager_llm_provider=manager_provider,
        architect_provider=architect_provider,
        auditor_provider=auditor_provider,
        verbose=verbose,
        use_multiple_auditors=use_multiple_auditors,
        enable_phase4_export=enable_phase4_export,
        phase4_output_path=phase4_output_path,
        phase4_enforce_quality_gate=phase4_enforce_quality_gate,
    )

    workflow = HierarchicalWorkflow(config)
    return workflow.execute(user_requirement)


__all__ = [
    "HierarchicalWorkflow",
    "HierarchicalWorkflowConfig",
    "execute_hierarchical_workflow",
]
